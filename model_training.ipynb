{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf2\n",
    "assert tf2.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Manager\n",
    "import random\n",
    "import os\n",
    "from scipy.integrate import quad_vec  # quad_vec allows to compute integrals accurately\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import qmc # to perform Latin Hypercube Sampling (LHS) \n",
    "import pandas as pd \n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs\n",
    "tf = tf2.compat.v1\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "real_type = tf.float32\n",
    "\n",
    "## Load the dataset\n",
    "feller_data = pd.read_csv('data//feller_d2.csv')\n",
    "\n",
    "## Obtain 2nd order differentials and their PCs\n",
    "second_differentials = feller_data.iloc[:,-36:]\n",
    "normalized_zscore = (second_differentials - second_differentials.mean()) / second_differentials.std()\n",
    "pca_d2 = PCA(n_components=15)\n",
    "second_differential_label = pca_d2.fit_transform(normalized_zscore[:9000])\n",
    "\n",
    "## Train-test Split (90-10)\n",
    "feller_testing = feller_data.iloc[9000:]\n",
    "feller_data = feller_data.iloc[:9000]\n",
    "\n",
    "training_col = feller_data.columns[:8]\n",
    "training_target = feller_data.columns[9]\n",
    "network_inputs = feller_data[training_col].values\n",
    "network_outputs = feller_data[training_target].values\n",
    "\n",
    "func_names = [\"lm\", \"r\", \"tau\", \"theta\", \"sigma\", \"rho\", \"kappa\", \"v0\"]\n",
    "network_inputs = feller_data[func_names]\n",
    "option_prices = feller_data['P_hat']\n",
    "network_first_order = feller_data[[f\"diff wrt {col}\" for col in func_names]].values\n",
    "sec_order_names = []\n",
    "for i in func_names:\n",
    "    for j in func_names:\n",
    "        if os.path.exists(f\"data//d2_{i}_{j}.csv\"):\n",
    "            sec_order_names.append(f\"d2_{i}_{j}\")\n",
    "network_second_order = feller_data[[f\"{col}\" for col in sec_order_names]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider four models:\n",
    "- Model 1: The benchmark framework \n",
    "    \n",
    "    (Heston parameters $\\mapsto$ option price)\n",
    "- Model 2: Model trained with 1st order differentials \n",
    "\n",
    "    (Heston parameters $\\mapsto$ (option price & 1st order differentials))\n",
    "- Model 3: Model trained with 1st and 2nd order differentials \n",
    "\n",
    "    (Heston parameters $\\mapsto$ (option price & 1st and 2nd differentials))\n",
    "- Model 4: Model trained with 1st order differentials, and 2nd order Diff-PCA differentials \n",
    "\n",
    "    (Heston parameters $\\mapsto$ (option price & 1st order differentials, 2nd order Diff-PCA differentials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Without Differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twin_net_with_first_order(hidden_units=64, hidden_layers=3):\n",
    "    raw_inputs = tf.keras.Input(shape=(8,))\n",
    "    x = raw_inputs\n",
    "    # Hidden layers\n",
    "    for _ in range(hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_units, activation='softplus')(x)\n",
    "    # Output layer (option price)\n",
    "    option_price = tf.keras.layers.Dense(1)(x)  # Predicted option price\n",
    "    # Create model with two outputs: option price and second-order differential\n",
    "    model = tf.keras.Model(inputs=raw_inputs, outputs=[option_price])\n",
    "    return model\n",
    "\n",
    "# Function to compute gradients and return price, first-order, and second-order differentials\n",
    "def compute_grad_model1(model, raw_inputs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)  # Get the outputs (option price, second-order diff)\n",
    "    \n",
    "    # Compute the first-order differential (gradient of option price w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return option_price, first_order_differential\n",
    "\n",
    "def compute_first_order_differential(model, raw_inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)[0]  # Get the option price (y_pred[0])\n",
    "    \n",
    "    # Compute the first-order differential (gradients w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return first_order_differential\n",
    "\n",
    "# Loss function that uses true gradients and predicted gradients\n",
    "def loss_fn(y_true, y_pred, true_first_order_differentials):\n",
    "    # Extract the true and predicted option price and second-order differentials\n",
    "    true_option_price = y_true[0]  # The first element contains the true option price\n",
    "    pred_option_price = y_pred[0]  # The first element contains the predicted option price\n",
    "    pred_first_order_diff = y_pred[1]\n",
    "\n",
    "    # Option price loss (L2 loss)\n",
    "    price_loss = tf.reduce_mean(tf.square(true_option_price - pred_option_price))  \n",
    "    # First-order differential loss (L2 loss)\n",
    "    first_order_loss = tf.reduce_mean(tf.square(true_first_order_differentials - pred_first_order_diff))\n",
    "\n",
    "    # Total loss (could be a weighted sum of the individual losses)\n",
    "    total_loss = price_loss + 0.5 * first_order_loss \n",
    "\n",
    "    # Return all losses (can be used for monitoring during training)\n",
    "    return total_loss, price_loss, first_order_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Generate some random input data (raw inputs)\n",
    "raw_inputs = tf.convert_to_tensor(network_inputs, dtype=tf.float32)\n",
    "\n",
    "# Generate some dummy true data for loss calculation (real data would come from a model/simulation)\n",
    "true_prices = tf.convert_to_tensor(option_prices.values.reshape(1,-1), dtype=tf.float32)\n",
    "true_first_order_differentials = tf.convert_to_tensor(network_first_order, dtype=tf.float32)\n",
    "true_second_order_differentials = tf.convert_to_tensor(network_second_order, dtype=tf.float32)\n",
    "\n",
    "# Create the model\n",
    "model = twin_net_with_first_order()\n",
    "history_1 = {\n",
    "    'total_loss': [],\n",
    "    'price_loss': [],\n",
    "    'first_order_loss': [],\n",
    "    'second_order_loss': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):  # Number of epochs\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get predictions (tuple of price, first-order diff, second-order diff)\n",
    "        predicted_price, predicted_first_order = compute_grad_model1(model, raw_inputs)\n",
    "        \n",
    "        # Compute loss (pass the true gradients as part of the loss function)\n",
    "        total_loss, price_loss, first_order_loss = loss_fn(\n",
    "            [true_prices, true_second_order_differentials],  # True data\n",
    "            [predicted_price, predicted_first_order],  # Model predictions\n",
    "            true_first_order_differentials  # True first-order differentials\n",
    "        )\n",
    "    \n",
    "    # Compute gradients with respect to model parameters\n",
    "    gradients = tape.gradient(price_loss, model.trainable_variables)\n",
    "    \n",
    "    # Update model parameters using the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    history_1['total_loss'].append(total_loss.numpy())\n",
    "    history_1['price_loss'].append(price_loss.numpy())\n",
    "    history_1['first_order_loss'].append(first_order_loss.numpy())\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"  Total Loss: {total_loss.numpy()}\")\n",
    "        print(f\"  Price Loss: {price_loss.numpy()}\")\n",
    "        print(f\"  First-Order Loss: {first_order_loss.numpy()}\")\n",
    "\n",
    "model.save(f\"results//model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Training with first order differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twin_net_with_first_order(hidden_units=64, hidden_layers=3):\n",
    "    raw_inputs = tf.keras.Input(shape=(8,))\n",
    "    x = raw_inputs\n",
    "    # Hidden layers\n",
    "    for _ in range(hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_units, activation='softplus')(x)\n",
    "    \n",
    "    # Output layer (option price)\n",
    "    option_price = tf.keras.layers.Dense(1)(x)  # Predicted option price\n",
    "    \n",
    "    # Create model with two outputs: option price and second-order differential\n",
    "    model = tf.keras.Model(inputs=raw_inputs, outputs=[option_price])\n",
    "    return model\n",
    "\n",
    "# Function to compute gradients and return price, first-order, and second-order differentials\n",
    "def compute_grad_model2(model, raw_inputs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)  # Get the outputs (option price, second-order diff)\n",
    "    \n",
    "    # Compute the first-order differential (gradient of option price w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return option_price, first_order_differential\n",
    "\n",
    "def compute_first_order_differential(model, raw_inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)[0]  # Get the option price (y_pred[0])\n",
    "    \n",
    "    # Compute the first-order differential (gradients w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return first_order_differential\n",
    "# Loss function that uses true gradients and predicted gradients\n",
    "def loss_fn(y_true, y_pred, true_first_order_differentials, lambda1):\n",
    "    # Extract the true and predicted option price and second-order differentials\n",
    "    true_option_price = y_true[0]  # The first element contains the true option price\n",
    "    pred_option_price = y_pred[0]  # The first element contains the predicted option price\n",
    "    pred_first_order_diff = y_pred[1]\n",
    "\n",
    "    # Option price loss (L2 loss)\n",
    "    price_loss = tf.reduce_mean(tf.square(true_option_price - pred_option_price))  \n",
    "    # First-order differential loss (L2 loss)\n",
    "    first_order_loss = tf.reduce_mean(tf.square(true_first_order_differentials - pred_first_order_diff))\n",
    "    # Total loss (could be a weighted sum of the individual losses)\n",
    "    total_loss = price_loss + lambda1 * first_order_loss \n",
    "    \n",
    "    # Return all losses (can be used for monitoring during training)\n",
    "    return total_loss, price_loss, first_order_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Generate some random input data (raw inputs)\n",
    "raw_inputs = tf.convert_to_tensor(network_inputs, dtype=tf.float32)\n",
    "\n",
    "# Generate some dummy true data for loss calculation (real data would come from a model/simulation)\n",
    "true_prices = tf.convert_to_tensor(option_prices.values.reshape(1,-1), dtype=tf.float32)\n",
    "true_first_order_differentials = tf.convert_to_tensor(network_first_order, dtype=tf.float32)\n",
    "true_second_order_differentials = tf.convert_to_tensor(network_second_order, dtype=tf.float32)\n",
    "\n",
    "# Create the model\n",
    "model = twin_net_with_first_order()\n",
    "history_2 = {\n",
    "    'total_loss': [],\n",
    "    'price_loss': [],\n",
    "    'first_order_loss': [],\n",
    "    'second_order_loss': [],\n",
    "    'lambda1': [],\n",
    "}\n",
    "# Training loop\n",
    "for lambda1 in [0.1,0.5,0.7]:\n",
    "    for epoch in range(1000):  # Number of epochs\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get predictions (tuple of price, first-order diff, second-order diff)\n",
    "            predicted_price, predicted_first_order = compute_grad_model2(model, raw_inputs)\n",
    "            \n",
    "            # Compute loss (pass the true gradients as part of the loss function)\n",
    "            total_loss, price_loss, first_order_loss = loss_fn(\n",
    "                [true_prices, true_second_order_differentials],  # True data\n",
    "                [predicted_price, predicted_first_order],  # Model predictions\n",
    "                true_first_order_differentials,  # True first-order differentials\n",
    "                lambda1\n",
    "            )\n",
    "        \n",
    "        # Compute gradients with respect to model parameters\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        \n",
    "        # Update model parameters using the optimizer\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        history_2['total_loss'].append(total_loss.numpy())\n",
    "        history_2['price_loss'].append(price_loss.numpy())\n",
    "        history_2['first_order_loss'].append(first_order_loss.numpy())\n",
    "        history_2['lambda1'].append(lambda1)\n",
    "        # Print the loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  Total Loss: {total_loss.numpy()}\")\n",
    "            print(f\"  Price Loss: {price_loss.numpy()}\")\n",
    "            print(f\"  First-Order Loss: {first_order_loss.numpy()}\")\n",
    "    model.save(f\"results//model2_{int(lambda1*10)}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Training with second order differentials wihtout Diff-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def twin_net_with_first_second_order(input_dim, hidden_units=64, hidden_layers=3):\n",
    "    raw_inputs = tf.keras.Input(shape=(8,))\n",
    "    x = raw_inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for _ in range(hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_units, activation='softplus')(x)\n",
    "    \n",
    "    # Output layer (option price)\n",
    "    option_price = tf.keras.layers.Dense(1)(x)  # Predicted option price\n",
    "    second_order_diff = tf.keras.layers.Dense(input_dim)(x)  # Predicted second-order differential (36 elements)\n",
    "    \n",
    "    # Create model with two outputs: option price and second-order differential\n",
    "    model = tf.keras.Model(inputs=raw_inputs, outputs=[option_price, second_order_diff])\n",
    "    return model\n",
    "\n",
    "# Function to compute gradients and return price, first-order, and second-order differentials\n",
    "def compute_grad_model3(model, raw_inputs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price, second_order_diff = model(raw_inputs)  # Get the outputs (option price, second-order diff)\n",
    "    \n",
    "    # Compute the first-order differential (gradient of option price w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return option_price, first_order_differential, second_order_diff\n",
    "\n",
    "def compute_first_order_differential(model, raw_inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)[0]  # Get the option price (y_pred[0])\n",
    "    \n",
    "    # Compute the first-order differential (gradients w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return first_order_differential\n",
    "# Loss function that uses true gradients and predicted gradients\n",
    "def loss_fn(y_true, y_pred, true_first_order_differentials, lambda1, lambda2):\n",
    "    # Extract the true and predicted option price and second-order differentials\n",
    "    true_option_price = y_true[0]  # The first element contains the true option price\n",
    "    true_second_order_diff = y_true[1]  # The remaining elements contain the true second-order differentials\n",
    "    pred_option_price = y_pred[0]  # The first element contains the predicted option price\n",
    "    pred_second_order_diff = y_pred[2]  # The second element contains the predicted second-order differentials\n",
    "    pred_first_order_diff = y_pred[1]\n",
    "    # Option price loss (L2 loss)\n",
    "    price_loss = tf.reduce_mean(tf.square(true_option_price - pred_option_price))  \n",
    "    \n",
    "    # print(true_second_order_diff,pred_second_order_diff)\n",
    "    # Second-order differential loss (L2 loss)\n",
    "    second_order_loss = tf.reduce_mean(tf.square(true_second_order_diff - pred_second_order_diff))\n",
    "    \n",
    "    # First-order differential loss (L2 loss)\n",
    "    first_order_loss = tf.reduce_mean(tf.square(true_first_order_differentials - pred_first_order_diff))\n",
    "\n",
    "    # Total loss (could be a weighted sum of the individual losses)\n",
    "    total_loss = price_loss + lambda1 * first_order_loss + lambda2* second_order_loss\n",
    "    \n",
    "    # Return all losses (can be used for monitoring during training)\n",
    "    return total_loss, price_loss, first_order_loss, second_order_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Generate some random input data (raw inputs)\n",
    "raw_inputs = tf.convert_to_tensor(network_inputs, dtype=tf.float32)\n",
    "\n",
    "# Generate some dummy true data for loss calculation (real data would come from a model/simulation)\n",
    "true_prices = tf.convert_to_tensor(option_prices.values.reshape(1,-1), dtype=tf.float32)\n",
    "true_first_order_differentials = tf.convert_to_tensor(network_first_order, dtype=tf.float32)\n",
    "true_second_order_differentials = tf.convert_to_tensor(network_second_order, dtype=tf.float32)\n",
    "\n",
    "# Create the model\n",
    "model = twin_net_with_first_second_order(network_second_order.shape[1])\n",
    "history_3 = {\n",
    "    'total_loss': [],\n",
    "    'price_loss': [],\n",
    "    'first_order_loss': [],\n",
    "    'second_order_loss': [],\n",
    "    'lambda1': [],\n",
    "    'lambda2': []\n",
    "}\n",
    "# Training loop\n",
    "for lambda1 in [0.1,0.5,0.7]:\n",
    "    for lambda2 in [0.1,0.5,0.7]:\n",
    "        for epoch in range(1000):  # Number of epochs\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get predictions (tuple of price, first-order diff, second-order diff)\n",
    "                predicted_price, predicted_first_order, predicted_second_order = compute_grad_model3(model, raw_inputs)\n",
    "                \n",
    "                # Compute loss (pass the true gradients as part of the loss function)\n",
    "                total_loss, price_loss, first_order_loss, second_order_loss = loss_fn(\n",
    "                    [true_prices, true_second_order_differentials],  # True data\n",
    "                    [predicted_price, predicted_first_order, predicted_second_order],  # Model predictions\n",
    "                    true_first_order_differentials,  # True first-order differentials\n",
    "                    lambda1, lambda2\n",
    "                )\n",
    "            \n",
    "            # Compute gradients with respect to model parameters\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            \n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            history_3['total_loss'].append(total_loss.numpy())\n",
    "            history_3['price_loss'].append(price_loss.numpy())\n",
    "            history_3['first_order_loss'].append(first_order_loss.numpy())\n",
    "            history_3['second_order_loss'].append(second_order_loss.numpy())\n",
    "            history_3['lambda1'].append(lambda1)\n",
    "            history_3['lambda2'].append(lambda2)\n",
    "            # Print the loss every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}:\")\n",
    "                print(f\"  Total Loss: {total_loss.numpy()}\")\n",
    "                print(f\"  Price Loss: {price_loss.numpy()}\")\n",
    "                print(f\"  First-Order Loss: {first_order_loss.numpy()}\")\n",
    "                print(f\"  Second-Order Loss: {second_order_loss.numpy()}\")\n",
    "        model.save(f\"results//model3_{int(lambda1*10)}_{int(lambda2*10)}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4: Training with second order differentials with Diff-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def twin_net_with_first_second_order(input_dim, hidden_units=64, hidden_layers=3):\n",
    "    raw_inputs = tf.keras.Input(shape=(8,))\n",
    "    x = raw_inputs\n",
    "\n",
    "    # Hidden layers\n",
    "    for _ in range(hidden_layers):\n",
    "        x = tf.keras.layers.Dense(hidden_units, activation='softplus')(x)\n",
    "    \n",
    "    # Output layer (option price)\n",
    "    option_price = tf.keras.layers.Dense(1)(x)  # Predicted option price\n",
    "    second_order_diff = tf.keras.layers.Dense(input_dim)(x)  # Predicted second-order differential (36 elements)\n",
    "    \n",
    "    # Create model with two outputs: option price and second-order differential\n",
    "    model = tf.keras.Model(inputs=raw_inputs, outputs=[option_price, second_order_diff])\n",
    "    return model\n",
    "\n",
    "# Function to compute gradients and return price, first-order, and second-order differentials\n",
    "def compute_grad_model4(model, raw_inputs):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price, second_order_diff = model(raw_inputs)  # Get the outputs (option price, second-order diff)\n",
    "    \n",
    "    # Compute the first-order differential (gradient of option price w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return option_price, first_order_differential, second_order_diff\n",
    "\n",
    "def compute_first_order_differential(model, raw_inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(raw_inputs)  # Watch the raw inputs\n",
    "        option_price = model(raw_inputs)[0]  # Get the option price (y_pred[0])\n",
    "    \n",
    "    # Compute the first-order differential (gradients w.r.t raw inputs)\n",
    "    first_order_differential = tape.gradient(option_price, raw_inputs)\n",
    "    \n",
    "    return first_order_differential\n",
    "# Loss function that uses true gradients and predicted gradients\n",
    "def loss_fn(y_true, y_pred, true_first_order_differentials, lambda1, lambda2):\n",
    "    # Extract the true and predicted option price and second-order differentials\n",
    "    true_option_price = y_true[0]  # The first element contains the true option price\n",
    "    true_second_order_diff = y_true[1]  # The remaining elements contain the true second-order differentials\n",
    "    pred_option_price = y_pred[0]  # The first element contains the predicted option price\n",
    "    pred_second_order_diff = y_pred[2]  # The second element contains the predicted second-order differentials\n",
    "    pred_first_order_diff = y_pred[1]\n",
    "    # Option price loss (L2 loss)\n",
    "    price_loss = tf.reduce_mean(tf.square(true_option_price - pred_option_price))  \n",
    "    \n",
    "    # print(true_second_order_diff,pred_second_order_diff)\n",
    "    # Second-order differential loss (L2 loss)\n",
    "    second_order_loss = tf.reduce_mean(tf.square(true_second_order_diff - pred_second_order_diff))\n",
    "    \n",
    "    # First-order differential loss (L2 loss)\n",
    "    first_order_loss = tf.reduce_mean(tf.square(true_first_order_differentials - pred_first_order_diff))\n",
    "    # Total loss (could be a weighted sum of the individual losses)\n",
    "    total_loss = price_loss + lambda1 * first_order_loss + lambda2*second_order_loss\n",
    "    \n",
    "    # Return all losses (can be used for monitoring during training)\n",
    "    return total_loss, price_loss, first_order_loss, second_order_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Generate some random input data (raw inputs)\n",
    "raw_inputs = tf.convert_to_tensor(network_inputs, dtype=tf.float32)\n",
    "\n",
    "# Generate some dummy true data for loss calculation (real data would come from a model/simulation)\n",
    "true_prices = tf.convert_to_tensor(option_prices.values.reshape(1,-1), dtype=tf.float32)\n",
    "true_first_order_differentials = tf.convert_to_tensor(network_first_order, dtype=tf.float32)\n",
    "true_second_order_differentials = tf.convert_to_tensor(second_differential_label, dtype=tf.float32)\n",
    "\n",
    "# Create the model\n",
    "model = twin_net_with_first_second_order(second_differential_label.shape[1])\n",
    "history_4 = {\n",
    "    'total_loss': [],\n",
    "    'price_loss': [],\n",
    "    'first_order_loss': [],\n",
    "    'second_order_loss': [],\n",
    "    'lambda1': [],\n",
    "    'lambda2': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for lambda1 in [0.1,0.5,0.7]:\n",
    "    for lambda2 in [0.1,0.5,0.7]:\n",
    "        for epoch in range(1000):  # Number of epochs\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get predictions (tuple of price, first-order diff, second-order diff)\n",
    "                predicted_price, predicted_first_order, predicted_second_order = compute_grad_model4(model, raw_inputs)\n",
    "                \n",
    "                # Compute loss (pass the true gradients as part of the loss function)\n",
    "                total_loss, price_loss, first_order_loss, second_order_loss = loss_fn(\n",
    "                    [true_prices, true_second_order_differentials],  # True data\n",
    "                    [predicted_price, predicted_first_order, predicted_second_order],  # Model predictions\n",
    "                    true_first_order_differentials,  # True first-order differentials\n",
    "                    lambda1, lambda2\n",
    "                )\n",
    "            \n",
    "            # Compute gradients with respect to model parameters\n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            \n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            history_4['total_loss'].append(total_loss.numpy())\n",
    "            history_4['price_loss'].append(price_loss.numpy())\n",
    "            history_4['first_order_loss'].append(first_order_loss.numpy())\n",
    "            history_4['second_order_loss'].append(second_order_loss.numpy())\n",
    "            history_4['lambda1'].append(lambda1)\n",
    "            history_4['lambda2'].append(lambda2)\n",
    "            # Print the loss every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}:\")\n",
    "                print(f\"  Total Loss: {total_loss.numpy()}\")\n",
    "                print(f\"  Price Loss: {price_loss.numpy()}\")\n",
    "                print(f\"  First-Order Loss: {first_order_loss.numpy()}\")\n",
    "                print(f\"  Second-Order Loss: {second_order_loss.numpy()}\")\n",
    "        model.save(f\"results//model4_{int(lambda1*10)}_{int(lambda2*10)}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for idx, history in enumerate([history_1,history_2,history_3,history_4], start=1):\n",
    "    max_length = max(len(v) for v in history.values() if v)\n",
    "\n",
    "    # Fill empty lists with NaNs\n",
    "    for key, value in history.items():\n",
    "        if len(value) < max_length:\n",
    "            history[key] = value + [np.nan] * (max_length - len(value))\n",
    "    history['index'] = list(range(len(next(iter(history.values())))))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(history)\n",
    "    df['source'] = f'history{idx}'  # Add a column identifying the source\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(combined_df)\n",
    "combined_df.to_csv(\"results//learning_history.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sys-trading",
   "language": "python",
   "name": "sys-trading"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
